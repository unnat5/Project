{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaration \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "In this project, using simulation I'm training an agent (double-jointed arm) that can move to target location.Thus the goal of agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "## Environment\n",
    "* **Set-up**: Double-jointed arm which can move to target locations.\n",
    "* **Goal**: The agent must move it's hand to goal location, and keep it there.\n",
    "* **Agents**: The environment contain 20 agent linked to a single brain.\n",
    "* **Agent Reward Function**(independent):\n",
    "  * +0.1 Each step agent's hand is in goal location.\n",
    "* **Brains**: One Brain with the following observation/action space.\n",
    "* **Vector Observation space** : 33 variables corresponding to position, rotation, velocity, and angular velocities of the two arm Rigidbodies.\n",
    "* Vector Action space (Continuous) size of 4, corresponding to torque applicable to two joints.\n",
    "* Benchmark Mean Reward: 30\n",
    "\n",
    "## Benchmark\n",
    "The environment is considered solved, when average reward (over 100 episode) of average score is at least +30. To calculate average score, after each episode, we add up the rewards taht each agent received (without discounting), to get a score for each agent. This yields 20 (potentially different) scores. We then take average of these 20 scores.\n",
    "\n",
    "<img src = a.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm used for training the Agent -- DDPG\n",
    "* To the train the agent I have used [DDPG](https://arxiv.org/pdf/1509.02971.pdf).\n",
    "* DDPG is classified as an Actor-Critc method.\n",
    "* And it chooses optimal action determinstically.\n",
    "* DDPG is best classified as a DQN method for continuous action spaces. \n",
    "* In **DDPG** we want to output the best believe action every time we query the action from the network.(That is determinstic policy)\n",
    "    * But by adding __noise__ in the action space we can force agent to explore more.\n",
    "\n",
    "<img src= b.png>\n",
    "\n",
    "#### Actor\n",
    "* Now actor here is used to approximate **optimal policy determinstically**\n",
    "* The actor is basically learning to output `argmax_aQ(s,a)` which is the best action.\n",
    "* This equivalent to policy gradient method approach where we directly estimate the policy of the environment with a neural network.\n",
    "* But one important thing to notice here is unlike policy gradient method where we collect trajectories and then compute cumulative future reward and then have a noisy MonteCarlo estimate(high variance) across which we calculate gradient and then optimize the network.\n",
    "* In DDPG we have CRITIC which estimates optimal value function, and with help of this we compute gradinet for actor and optimize it's weight. \n",
    "\n",
    "#### Critic\n",
    "* Learns to evaluate the optimal action value function by using **ACTOR** best believe action.\n",
    "\n",
    "\n",
    "## Pipeline for DDPG\n",
    "* We have four network\n",
    "    1. `actor_local`\n",
    "    2. `actor_target`\n",
    "    3. `critic_local`\n",
    "    4. `critic_target`\n",
    "* First step is we collect our experience tuples which consisits of `(state, action, reward, next_state, done)`.\n",
    "* We randomly initialize (xavier initialization) our network. One of the key component which I used and found very helpful in this problem was initializing local and target networks with same set of random weights.\n",
    "* We first get a state from environment and then we pass it through `actor_local` and get an action, by taking this action we get next_state and reward. By this process we collect experience tuples and push it in replay buffer.\n",
    "* After our replay buffer length is greater or equal to batch_size, we randomly sample from replay buffer to break sequential correlation.\n",
    "* The fact we have four network is because while training our neural network we need targets, so that we can compute loss and then gradient across it. But in RL target itself is a function of weights, so we want to break this relation that is the reason we have target network which are different from local network, but with time we do weighted sum of local network and target network weights and assign them to target network and this is known as **soft update**.\n",
    "    * The soft update consists of slowly blending our regular network weights with our target weights.\n",
    "        * Every step, mix  0.010.01  of regular network weights with target network weights.\n",
    "\n",
    "\n",
    "### Update Critic\n",
    "* In DDPG paper critic maps **Q-value**, and in critic network we input both state vector and action vector.\n",
    "* So in critic network we want to output optimal **action value** and we know this from **Temporal difference algorithm** that `Q(s,a)= r + γ * max_a Q(next state,a)`.\n",
    "* So in a Critic network:\n",
    "    * Predicted_value : ` Q_expected = critic_local(state,action) `\n",
    "    * Target_value : `r + γ * critic_target(next_state,actor_target(next_state))`\n",
    "* This way of training critic network is pushing actor network to output the optimal action, in other words the action which maximize the action value.\n",
    "* And one important thing to notice is that we want our critic to be somewhat bias but should have low **variance** that is the reason we use temporal difference algorithm to compute our target.\n",
    "* And as Actor has a high variance, with critic's output we train Actor, the main idea is to reduce the variance in actor network so we can train the network.\n",
    "* Advance algorithm like A3C,PPO all want to reduce the variance problem in RL agent.\n",
    "\n",
    "\n",
    "### Update Actor\n",
    "* In Actor network we input the state and get the action vector. (estimates Policy)\n",
    "* In main idea behind actor training is that, we want actor network to output such action that maximizes the action value function which is estimated by critic.\n",
    "* So in Actor network:\n",
    "    * Loss: `-critic_local(state,actor_local(state))` -- gradient is calculated across this loss.\n",
    "    * minus sign because we want to do gradient ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "#### model.py\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## code is inspired from https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum\n",
    "\n",
    "\n",
    "def hidden_init(layer):\n",
    "    \"\"\"\n",
    "        xavier initialization.\n",
    "    \"\"\"\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1./np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, fc1_units =200, fc2_units=150):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__() ## initialize the nn.Module class\n",
    "        self.seed = torch.manual_seed(random_seed)\n",
    "        self.fc2 = nn.Linear(state_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units,action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        #self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps state -> actions\"\"\"\n",
    "        x = F.leaky_relu(self.fc2(state))\n",
    "        return F.tanh(self.fc3(x))\n",
    "    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\" Critic (value) model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, fcs1_units=200, fc2_units=150):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size(int): Dimension of each state\n",
    "            action_size(int): Dimension of each action\n",
    "            seed (init): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__() ##initialize the nn.Module class\n",
    "        self.seed = torch.manual_seed(random_seed)\n",
    "        self.fcs1 = nn.Linear(state_size,fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units,1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self,state,action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.leaky_relu(self.fcs1(state))\n",
    "        x = torch.cat((xs,action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "```\n",
    "#### ddpg_agnet.py\n",
    "```python\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from model import Actor, Critic\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "## code is inspired from https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum\n",
    "\n",
    "##========== HYPERPARAMETER ============##\n",
    "BUFFER_SIZE = int(1e5)    # replay buffer\n",
    "BATCH_SIZE = 128          # minibatch size\n",
    "GAMMA = 0.99              # discounting factor\n",
    "TAU = 1e-3                # soft update of traget parameters\n",
    "LR_ACTOR = 1e-3           # learning rate for actor\n",
    "LR_CRITIC = 1e-3          # learning rate for critic\n",
    "WEIGHT_DECAY = 0.         # L2 weight weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment\"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size \n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr = LR_ACTOR)\n",
    "        \n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size,action_size,random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size,action_size,random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr = LR_CRITIC, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size,random_seed)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "        self.counter = 0\n",
    "        \n",
    "       # Make sure target is with the same weight as the source found on slack\n",
    "        self.hard_update(self.actor_target, self.actor_local)\n",
    "        self.hard_update(self.critic_target, self.critic_local)\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward \n",
    "        for state,action,reward,next_state,done in zip(state, action, reward, next_state, done):\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "            self.counter+=1\n",
    "        \n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE and self.counter%10==0: \n",
    "            experience = self.memory.sample()\n",
    "            self.learn(experience, GAMMA)\n",
    "            \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Return actions for given state as per current policy.\"\"\"\n",
    "        #Save experience / reward\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experience, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples\n",
    "        \n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        \n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state,action) -> Q-value\n",
    "            \n",
    "        Params\n",
    "        ======\n",
    "            experience (Torch[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        state, action, reward, next_state, done = experience\n",
    "        \n",
    "        # ============================== Update Critic =================================#\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        \n",
    "        self.actor_target.eval() ## there is no point is saving gradient\n",
    "        self.critic_target.eval()\n",
    "        \n",
    "        actions_next = self.actor_target(next_state)\n",
    "        Q_target_next = self.critic_target(next_state,actions_next)\n",
    "        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = reward + (gamma*Q_target_next*(1-done))\n",
    "        \n",
    "        ## Compute Critic Loss\n",
    "        Q_expected = self.critic_local(state,action)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        ## Minize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ============================== Update Actor =================================#\n",
    "        ## Compute actor loss\n",
    "        action_pred  = self.actor_local(state)\n",
    "        actor_loss = -(self.critic_local(state,action_pred).mean())\n",
    "        ## The reason we can calculate loss this way and we don't have\n",
    "        ## to collect trajector ( noisy Monte carlo estimation; cum_reward/reward_future)\n",
    "        ## is action space is continuous and differentiable and we calculate\n",
    "        ## gradient w.r.t to q_value which is estimated by CRITIC.\n",
    "        # Minimize loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # ========================== Update target network =================================#\n",
    "\n",
    "        self.soft_update(self.critic_local,self.critic_target,TAU)\n",
    "        self.soft_update(self.actor_local,self.actor_target,TAU)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param,local_param in zip(target_model.parameters(),\n",
    "                                           local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n",
    "\n",
    "    \n",
    "    def hard_update(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "    \n",
    "    \n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, seed, mu=0.01, theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu*np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta*(self.mu-x) + self.sigma*np.array([random.gauss(0., 1.) for i in range(len(x))])\n",
    "        self.state =x +dx\n",
    "        return self.state\n",
    "    \n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "        \n",
    "```\n",
    "\n",
    "#### Training\n",
    "```python\n",
    "from ddpg_agent import Agent\n",
    "random_seed=8\n",
    "agent = Agent(state_size,action_size,random_seed)\n",
    "\n",
    "from logger import Logger\n",
    "from collections import deque\n",
    "import torch\n",
    "def ddpg(n_episode=300, max_t=320, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scoress= []\n",
    "    logger = Logger('./logs')\n",
    "    for i_episodes in range(1, n_episode+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        for ii in range(1,1001):\n",
    "            actions=[agent.act(states[no_agent,:]) for no_agent in range(20)]\n",
    "            actions = np.array(actions).reshape(20,4)\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            agent.step(states,actions,rewards,next_states,dones)\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        mean_reward = np.mean(scores)\n",
    "        scores_deque.append(mean_reward)\n",
    "        scoress.append(mean_reward)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}  Best_score: {} current score: {}'.format(i_episodes,np.mean(scores_deque),\n",
    "                                                                                             max(scores_deque),scores_deque[-1]),end=\"\")\n",
    "        #torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        #torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episodes%print_every ==0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episodes,np.mean(scores_deque)))\n",
    "            \n",
    "            \n",
    "        # =============================================================================== #\n",
    "        #                           Tensorboard Logging                                   #\n",
    "        # =============================================================================== #\n",
    "        \n",
    "        # 1. Log scalar values (scalar summary)\n",
    "        info = {'Average_Score_episode(over-100)': np.mean(scores_deque),\n",
    "                'Average_Score_acrossAgent': mean_reward}\n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag, value, i_episodes)\n",
    "            \n",
    "        # 2. Log value and gradient of the parameters (histogram summary)\n",
    "        ## Actor_local \n",
    "        for tag, value in agent.actor_local.named_parameters():\n",
    "            tag = tag.replace('.','/')\n",
    "            logger.histo_summary('ActorLocal/' +tag, value.data.cpu().numpy(), i_episodes)\n",
    "            logger.histo_summary('ActorLocal/'+tag+'/grad', value.grad.data.cpu().numpy(),i_episodes)\n",
    "        ## Actor_target\n",
    "        for tag, value in agent.actor_target.named_parameters():\n",
    "            tag = tag.replace('.','/')\n",
    "            logger.histo_summary('ActorTarget/'+tag, value.data.cpu().numpy(), i_episodes)\n",
    "            logger.histo_summary('ActorTarget/'+tag+'/grad', value.grad.data.cpu().numpy(),i_episodes)\n",
    "            \n",
    "        ## Critic_local\n",
    "        for tag, value in agent.critic_local.named_parameters():\n",
    "            tag = tag.replace('.','/')\n",
    "            logger.histo_summary('CriticLocal/'+tag, value.data.cpu().numpy(), i_episodes)\n",
    "            logger.histo_summary('CriticLocal/'+tag+'/grad', value.grad.data.cpu().numpy(),i_episodes)\n",
    "        ## Critic_target\n",
    "        for tag, value in agent.critic_target.named_parameters():\n",
    "            tag = tag.replace('.','/')\n",
    "            logger.histo_summary('CriticTarget/'+tag, value.data.cpu().numpy(), i_episodes)\n",
    "            logger.histo_summary('CriticTarget/'+tag+'/grad', value.grad.data.cpu().numpy(),i_episodes)\n",
    "            \n",
    "            \n",
    "    return scoress\n",
    "scores = ddpg()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
